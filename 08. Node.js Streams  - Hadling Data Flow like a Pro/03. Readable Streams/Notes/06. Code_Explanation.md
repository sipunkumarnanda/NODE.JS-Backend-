### Explanation Notes for File Reading Using Streams in Node.js

---

## üìù **Overview**

This code demonstrates **reading a large file** using **streams** in Node.js, which is an efficient way to handle large files without consuming too much memory. In this example, the file is read in **chunks**, and each chunk is processed and written to another file. Streams allow us to **work with data progressively**, which is especially useful for large files.

---

### üí° **Key Concepts and Methods**

1. **Stream**: A stream allows data to be processed in small pieces (chunks) rather than loading the entire file into memory at once. This makes it ideal for large files that don't fit in memory.

2. **`fs.createReadStream`**: This method creates a readable stream for a file, allowing you to **read the file in chunks**. The `highWaterMark` option determines the size of each chunk.

3. **`data` Event**: This event is emitted whenever a chunk of data is available to be processed. We can attach a callback function to this event to handle the chunks.

4. **`end` Event**: This event is emitted when the stream has finished reading all the data. It indicates that no more data will be emitted by the stream.

---

### üîë **Code Breakdown**

```js
import fs from "fs"; // Importing the Node.js fs module to interact with the file system

console.time(); // Start measuring the time for the file reading process

// Create a readable stream for the large file with a custom buffer size (100MB)
const readStream = fs.createReadStream(
    "C:\\Users\\Sipun Kumar Nanda\\OneDrive\\Desktop\\Squid.Game.S02.720p.10Bit.WEB-DL.Hindi.5.1-English.2.0.ESub.x264-HDHub4u.Tv.zip",
    { highWaterMark: 100 * 1024 * 1024 } // Buffer size set to 100MB per chunk
);

let readCount = 0; // Variable to track the number of chunks read

// Attach a listener for the 'data' event, which is triggered when a chunk is available
readStream.on("data", (chunkBuffer) => {
    console.log(chunkBuffer);  // Logs the chunk of data (Buffer object)
    console.log(chunkBuffer.byteLength);  // Logs the size of the current chunk (in bytes)

    // Write the chunk to a new file using appendFileSync (this saves each chunk as it's read)
    fs.appendFileSync('data.zip', chunkBuffer);

    readCount++;  // Increment the count for each chunk read
});

// Attach a listener for the 'end' event, which is triggered when the stream finishes reading
readStream.on("end", () => {
    // This event is fired once the entire file has been read
    console.log("Read Count is:", readCount); // Logs the total number of chunks read
    console.timeEnd(); // End the timer and log the total time taken for the operation
});
```

---

### üß† **Detailed Explanation**

1. **`fs.createReadStream` with `highWaterMark`**  
   - This method creates a **readable stream** for the specified file.
   - The `highWaterMark` is set to **100MB**. This controls how large each chunk will be. The default value for `highWaterMark` is **64 KB**, but here we increase it to **100MB** for efficiency when reading large files.
   - Larger buffer sizes generally reduce the number of reads required (fewer events emitted), but it also uses more memory. If the buffer is too large, it may impact memory performance. So, it's important to choose an appropriate buffer size based on the file and available memory.

2. **`data` Event**  
   - The `data` event is triggered **each time** a new chunk of data is available.
   - Each `chunkBuffer` represents a **Buffer object** containing data read from the file.
   - We use `chunkBuffer.byteLength` to log the size of the chunk.
   - `fs.appendFileSync('data.zip', chunkBuffer)` writes the chunk to a new file named `data.zip` as it's read.
   - The `readCount` variable tracks how many chunks have been read and processed.

3. **`end` Event**  
   - The `end` event fires when the stream has finished reading all the data. This is where we log the **total chunks read** and **end the timer**.
   - It's a **signal** that the entire file has been read and processed, allowing us to perform any final operations like cleanup or logging.

---

### ‚è± **Performance**

- **Time Measurement**: The `console.time()` and `console.timeEnd()` methods measure how long the entire process takes.
- By using **streams**, we avoid reading the entire file into memory at once, which is especially helpful when working with **large files** (in this case, a zip file).

---

### üß© **Use Case of This Approach**

This approach is suitable for scenarios where:

- You need to **process large files** (e.g., video, audio, zip, database dumps) without consuming a lot of memory.
- You want to **write data incrementally** (like copying large files or processing data in chunks).
- The file is **too large to fit in memory** (e.g., files larger than 2GB).

---

### üöÄ **Advantages of Using Streams for File Operations**

- **Efficient Memory Usage**: Streams process data in smaller chunks, reducing the memory load compared to reading the entire file at once.
- **Non-blocking I/O**: Streams in Node.js are **asynchronous**, meaning the program can continue executing other operations while waiting for data to be processed.
- **Real-time Processing**: Streams allow for **real-time processing** of data, which is beneficial for things like video streaming, live data feeds, or large-scale data migrations.

---

### üìù **Takeaway Points**

- **Streams** are an efficient way to read and write large files by processing data in **chunks**.
- The **`data` event** is triggered for every chunk that is available to read.
- The **`end` event** signals when the entire stream has been processed.
- Using the **`highWaterMark`** option, you can control the buffer size and optimize performance based on your needs.

---