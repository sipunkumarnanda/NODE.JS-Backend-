## 🔍 **Understanding `readStream.on("data", callback)` and `readStream.on("end", callback)` in Node.js Streams**  

When working with **streams** in Node.js, we primarily use two important events:  

1️⃣ **`data` Event**: Triggered when a chunk of data is available for processing.  
2️⃣ **`end` Event**: Triggered when all data has been read from the stream.  

---

## 📌 **1. `readStream.on("data", callback)`**
### ✅ **What is it?**  
The `"data"` event fires **whenever a chunk of data is read from the stream**.  
- This event is emitted multiple times (depending on the file size).  
- Each emitted **chunk** is a **Buffer object**.  

### 📜 **Syntax**  
```js
import fs from "fs";

const readStream = fs.createReadStream("largeFile.txt", { highWaterMark: 64 * 1024 }); // 64 KB buffer

readStream.on("data", (chunkBuffer) => {
    console.log("Received data chunk:", chunkBuffer);
    console.log("Chunk Size:", chunkBuffer.byteLength, "bytes");
});
```
---

### 📍 **When to Use `data` Event?**
✔️ **Processing large files efficiently** (avoids memory overload).  
✔️ **Handling real-time data streams** (e.g., audio/video streaming).  
✔️ **Reading data progressively instead of loading the entire file into memory.**  

---

## 📌 **2. `readStream.on("end", callback)`**
### ✅ **What is it?**  
The `"end"` event fires when **all chunks have been read, and no more data remains** in the stream.  

### 📜 **Syntax**  
```js
readStream.on("end", () => {
    console.log("File read complete!");
});
```

---

### 📍 **When to Use `end` Event?**
✔️ **Finalizing processing once all data has been read.**  
✔️ **Closing database connections, cleaning up resources, etc.**  
✔️ **Logging when streaming operations are fully completed.**  

---

## 📊 **Comparison Table**
| Feature              | `data` Event | `end` Event |
|----------------------|-------------|-------------|
| When it is triggered | **Each time a chunk is available** | **When all chunks are read** |
| How many times does it execute? | **Multiple times** (once per chunk) | **Only once** |
| What does it return? | A **Buffer** (containing a chunk of data) | **No data, just a signal** |
| Use case | **Processing streaming data** (logs, videos, etc.) | **Finalizing after reading completes** |

---

## 🚀 **Example: Reading a Large File with Progress**
This example shows **both events in action** while reading a file and displaying **progress** in the terminal.  

```js
import fs from "fs";

const filePath = "largeFile.txt";
const readStream = fs.createReadStream(filePath, { highWaterMark: 100 * 1024 }); // 100 KB per chunk

let totalRead = 0;

readStream.on("data", (chunkBuffer) => {
    totalRead += chunkBuffer.byteLength;
    console.log(`Read ${chunkBuffer.byteLength} bytes. Total: ${totalRead} bytes`);
});

readStream.on("end", () => {
    console.log(`File reading complete! Total bytes read: ${totalRead}`);
});
```

🔹 **Output Example for a 500KB file (with 100KB chunks)**:  
```
Read 102400 bytes. Total: 102400 bytes
Read 102400 bytes. Total: 204800 bytes
Read 102400 bytes. Total: 307200 bytes
Read 102400 bytes. Total: 409600 bytes
Read 102400 bytes. Total: 512000 bytes
File reading complete! Total bytes read: 512000
```

---

## 🎯 **Key Takeaways**
✔️ Use `data` event to **process chunks** as they arrive.  
✔️ Use `end` event to **detect when streaming is fully completed**.  
✔️ Streams **save memory** compared to `fs.readFile()` since they don’t load the entire file at once.  
---